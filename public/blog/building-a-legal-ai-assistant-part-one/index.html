<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <title> | Building your own LLM &amp; RAG with Python (# 1: Tools and Setup)</title>
    
  
    <link rel="stylesheet" href="https://elvim.cc/style.css?h=501c6ea82db27d74ea03">
    
  <link rel="stylesheet" href="/css/custom.css">

    
        
    
</head>
<body>
    
<header class="space">
    <a href="https:&#x2F;&#x2F;elvim.cc&#x2F;">&LeftArrow; Home</a>
</header>

    
<main>
    <h1>Building your own LLM &amp; RAG with Python (# 1: Tools and Setup)</h1>
    
    <p class="secondary small">
        20 August, 2025

        
        
        
        
        

        
        
        - Categories:
        
        
        <a href="https:&#x2F;&#x2F;elvim.cc&#x2F;categories&#x2F;llm&#x2F;">LLM</a>
        
        
    </p>
    
    <div class="space"></div>
    <h1 id="building-an-ai-assistant-with-python">Building an AI Assistant with Python</h1>
<blockquote>
<p>⚠️ <strong>Disclaimer (read this before you hyperventilate about “AI”)</strong></p>
<p>I am <em>not</em> an “AI influencer” and I don’t spend my days asking coding assistants to write my for-loops.<br />
I am frankly nauseated by the hype train around this stuff.</p>
<p>Every corporate pitch deck these days seems to think “AI” is a magic solution that must be applied to everything from paper clips to lotion. It’s really not.</p>
<p>This technology is still immature, it makes mistakes, and it’s not magic fairy dust. As long as you use common sense while using LLM's thare are a useful tool.</p>
<p>That’s the spirit of this series: not hype, not marketing, but a practical experiment.</p>
</blockquote>
<h3 id="part-1-tools-and-setup">Part 1: Tools and Setup</h3>
<p>I will document how I built a small <em>AI-powered assistant</em> for the Danish <em>Almenlejeloven</em> (Housing Act). You can use these technologies for building RAGs for anything of course.</p>
<ul>
<li><strong>Part 1</strong> (this post): Introduces the tools and how to prepare the data.</li>
<li><strong>Part 2</strong>: Shows how to connect everything into a working Retrieval-Augmented Generation (RAG) system.</li>
</ul>
<hr />
<h2 id="why-build-this">Why Build This?</h2>
<p>Large Language Models (LLMs) like ChatGPT are powerful, but they have two issues when you want to use them with your own documents:</p>
<ol>
<li><strong>They don’t know your data</strong> – unless it was in their training set, they won’t have access to your exact law, manual, or policy.</li>
<li><strong>They can hallucinate</strong> – making up answers that sound correct but aren’t.<sup>Also, stop calling it "Hallucinate"</sup></li>
</ol>
<p>To solve this, we need two things:</p>
<ul>
<li>A way to <strong>store and search our own text data</strong> by meaning.</li>
<li>A way to <strong>run a language model locally</strong> and give it just the text it needs.</li>
</ul>
<p>That’s where <strong>Chroma</strong> and <strong>Ollama</strong> come in.</p>
<hr />
<h2 id="what-is-ollama">What is Ollama?</h2>
<p><a href="https://ollama.com/">Ollama</a> is a tool that lets you run open-source LLMs <em>locally</em> on your own machine.</p>
<ul>
<li>It works on macOS, Linux, and Windows.</li>
<li>It has its own model catalog: <code>llama2</code>, <code>mistral</code>, <code>gemma</code>, etc.</li>
<li>It provides a simple API on <code>http://localhost:11434</code> so you can send prompts from Python.</li>
</ul>
<p>Why use Ollama?</p>
<ul>
<li><strong>Privacy</strong>: Your questions and documents never leave your machine.</li>
<li><strong>Flexibility</strong>: You can try different models easily.</li>
<li><strong>Cost</strong>: No API keys or cloud bills.</li>
</ul>
<p>Example request to Ollama:</p>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#b48ead;">import </span><span>requests
</span><span>
</span><span>resp = requests.</span><span style="color:#bf616a;">post</span><span>(
</span><span>    &quot;</span><span style="color:#a3be8c;">http://localhost:11434/api/chat</span><span>&quot;,
</span><span>    </span><span style="color:#bf616a;">json</span><span>={
</span><span>        &quot;</span><span style="color:#a3be8c;">model</span><span>&quot;: &quot;</span><span style="color:#a3be8c;">llama3.1</span><span>&quot;,
</span><span>        &quot;</span><span style="color:#a3be8c;">messages</span><span>&quot;: [
</span><span>            {&quot;</span><span style="color:#a3be8c;">role</span><span>&quot;: &quot;</span><span style="color:#a3be8c;">system</span><span>&quot;, &quot;</span><span style="color:#a3be8c;">content</span><span>&quot;: &quot;</span><span style="color:#a3be8c;">You are a helpful assistant.</span><span>&quot;},
</span><span>            {&quot;</span><span style="color:#a3be8c;">role</span><span>&quot;: &quot;</span><span style="color:#a3be8c;">user</span><span>&quot;, &quot;</span><span style="color:#a3be8c;">content</span><span>&quot;: &quot;</span><span style="color:#a3be8c;">What is the capital of Denmark?</span><span>&quot;}
</span><span>        ]
</span><span>    }
</span><span>)
</span><span>
</span><span style="color:#96b5b4;">print</span><span>(resp.</span><span style="color:#bf616a;">json</span><span>()[&quot;</span><span style="color:#a3be8c;">message</span><span>&quot;][&quot;</span><span style="color:#a3be8c;">content</span><span>&quot;])
</span><span style="color:#65737e;"># -&gt; &quot;The capital of Denmark is Copenhagen.&quot;
</span></code></pre>
<p>I have an Nvidia GPU in my machine which let's me utilize CUDA - this speeds up things greatly compared to running an LLM on your CPU. YMMV.</p>
<hr />
<h2 id="what-is-chroma">What is Chroma?</h2>
<p><a href="https://www.trychroma.com/">Chroma</a> is a <strong>vector database</strong>. Instead of storing text as plain strings, it stores <strong>embeddings</strong>: numerical representations of meaning.</p>
<ul>
<li>If you ask about “rehousing” and your text says “erstatningsbolig”, embeddings help the database see that these are <em>semantically related</em>.</li>
<li>You can query Chroma with a piece of text, and it returns the most similar text chunks from your collection.</li>
</ul>
<p>Example usage:</p>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#b48ead;">import </span><span>chromadb
</span><span style="color:#b48ead;">from </span><span>chromadb.utils </span><span style="color:#b48ead;">import </span><span>embedding_functions
</span><span>
</span><span style="color:#65737e;"># Create a persistent client
</span><span>client = chromadb.</span><span style="color:#bf616a;">PersistentClient</span><span>(</span><span style="color:#bf616a;">path</span><span>=&quot;</span><span style="color:#a3be8c;">db</span><span>&quot;)
</span><span>
</span><span style="color:#65737e;"># Use a multilingual embedding model
</span><span>emb_fn = embedding_functions.</span><span style="color:#bf616a;">SentenceTransformerEmbeddingFunction</span><span>(
</span><span>    </span><span style="color:#bf616a;">model_name</span><span>=&quot;</span><span style="color:#a3be8c;">paraphrase-multilingual-MiniLM-L12-v2</span><span>&quot;
</span><span>)
</span><span>
</span><span style="color:#65737e;"># Create collection
</span><span>coll = client.</span><span style="color:#bf616a;">get_or_create_collection</span><span>(</span><span style="color:#bf616a;">name</span><span>=&quot;</span><span style="color:#a3be8c;">laws</span><span>&quot;, </span><span style="color:#bf616a;">embedding_function</span><span>=emb_fn)
</span><span>
</span><span style="color:#65737e;"># Add some documents (Paraphrased for brevity)
</span><span>coll.</span><span style="color:#bf616a;">add</span><span>(
</span><span>    </span><span style="color:#bf616a;">documents</span><span>=[
</span><span>        &quot;</span><span style="color:#a3be8c;">§ 86a: Temporary replacement housing must be suitable and in the same municipality.</span><span>&quot;,
</span><span>        &quot;</span><span style="color:#a3be8c;">§ 85: A landlord must offer a replacement home if a tenant is terminated.</span><span>&quot;
</span><span>    ],
</span><span>    </span><span style="color:#bf616a;">ids</span><span>=[&quot;</span><span style="color:#a3be8c;">sec86a</span><span>&quot;, &quot;</span><span style="color:#a3be8c;">sec85</span><span>&quot;]
</span><span>)
</span><span>
</span><span style="color:#65737e;"># Query
</span><span>res = coll.</span><span style="color:#bf616a;">query</span><span>(</span><span style="color:#bf616a;">query_texts</span><span>=[&quot;</span><span style="color:#a3be8c;">rehousing rules</span><span>&quot;], </span><span style="color:#bf616a;">n_results</span><span>=</span><span style="color:#d08770;">2</span><span>)
</span><span style="color:#96b5b4;">print</span><span>(res[&quot;</span><span style="color:#a3be8c;">documents</span><span>&quot;])
</span><span style="color:#65737e;"># -&gt; returns the most relevant law sections
</span></code></pre>
<hr />
<h2 id="preparing-the-danish-housing-act-markdown-chunks">Preparing the Danish Housing Act (Markdown → Chunks)</h2>
<p>The Danish Housing Act is available online and can be exported to Markdown format (You can google sites that convert website content to Markdown). I copied the entire Housing act as markdown and saved it into a file this way.
However, that file contains way to much text and is too messy for our use. We will need to:</p>
<ol>
<li>
<p><strong>Split by § (sections)</strong><br />
Each paragraph (§1, §2, §3 …) becomes its own file.</p>
</li>
<li>
<p><strong>Clean up Markdown</strong><br />
Remove links, images, ads, and keep only the text.</p>
</li>
<li>
<p><strong>Chunk the text</strong><br />
Long sections are further split into ~1200-character chunks. This is important because:</p>
<ul>
<li>LLMs have context size limits.</li>
<li>Smaller chunks improve retrieval accuracy.</li>
<li>Overlap between chunks prevents losing context between cuts.</li>
</ul>
</li>
</ol>
<p>For this project we can write some code that helps us split up our "Raw" data.<br />
Here's an example chunking function in Python:</p>
<pre data-lang="python" style="background-color:#2b303b;color:#c0c5ce;" class="language-python "><code class="language-python" data-lang="python"><span style="color:#b48ead;">import </span><span>re
</span><span>
</span><span style="color:#b48ead;">def </span><span style="color:#8fa1b3;">chunk_text</span><span>(</span><span style="color:#bf616a;">text</span><span>, </span><span style="color:#bf616a;">max_chars</span><span>=</span><span style="color:#d08770;">1200</span><span>, </span><span style="color:#bf616a;">overlap</span><span>=</span><span style="color:#d08770;">150</span><span>):
</span><span>    </span><span style="color:#65737e;"># Split the input text into paragraphs.
</span><span>    </span><span style="color:#65737e;"># We split on blank lines (\n\s*\n) and remove empty ones.
</span><span>    paras = [p.</span><span style="color:#bf616a;">strip</span><span>() </span><span style="color:#b48ead;">for </span><span>p </span><span style="color:#b48ead;">in </span><span>re.</span><span style="color:#bf616a;">split</span><span>(</span><span style="color:#b48ead;">r</span><span>&quot;</span><span style="color:#96b5b4;">\n</span><span style="color:#d08770;">\s</span><span>*</span><span style="color:#96b5b4;">\n</span><span>&quot;, text) </span><span style="color:#b48ead;">if </span><span>p.</span><span style="color:#bf616a;">strip</span><span>()]
</span><span>
</span><span>    </span><span style="color:#65737e;"># chunks = final list of text pieces
</span><span>    </span><span style="color:#65737e;"># buf = temporary buffer we build up until it&#39;s &quot;full enough&quot;
</span><span>    chunks, buf = [], &quot;&quot;
</span><span>
</span><span>    </span><span style="color:#65737e;"># Loop through each paragraph
</span><span>    </span><span style="color:#b48ead;">for </span><span>p </span><span style="color:#b48ead;">in </span><span>paras:
</span><span>        </span><span style="color:#65737e;"># Try to add this paragraph to the buffer
</span><span>        cand = (buf + &quot;</span><span style="color:#96b5b4;">\n\n</span><span>&quot; + p).</span><span style="color:#bf616a;">strip</span><span>() </span><span style="color:#b48ead;">if </span><span>buf </span><span style="color:#b48ead;">else </span><span>p
</span><span>
</span><span>        </span><span style="color:#65737e;"># If the combined buffer is still small enough, keep it
</span><span>        </span><span style="color:#b48ead;">if </span><span style="color:#96b5b4;">len</span><span>(cand) &lt;= max_chars:
</span><span>            buf = cand
</span><span>        </span><span style="color:#b48ead;">else</span><span>:
</span><span>            </span><span style="color:#65737e;"># Otherwise: the buffer is full, save it as a chunk
</span><span>            </span><span style="color:#b48ead;">if </span><span>buf:
</span><span>                chunks.</span><span style="color:#bf616a;">append</span><span>(buf)
</span><span>
</span><span>            </span><span style="color:#65737e;"># Now handle the current paragraph, which is too long
</span><span>            </span><span style="color:#65737e;"># Break it into hard slices of max_chars size
</span><span>            </span><span style="color:#65737e;"># (with overlap to avoid cutting sentences too harshly)
</span><span>            </span><span style="color:#b48ead;">for </span><span>i </span><span style="color:#b48ead;">in </span><span style="color:#96b5b4;">range</span><span>(</span><span style="color:#d08770;">0</span><span>, </span><span style="color:#96b5b4;">len</span><span>(p), max_chars - overlap):
</span><span>                chunks.</span><span style="color:#bf616a;">append</span><span>(p[i:i+max_chars])
</span><span>
</span><span>            </span><span style="color:#65737e;"># Reset buffer
</span><span>            buf = &quot;&quot;
</span><span>
</span><span>    </span><span style="color:#65737e;"># If there&#39;s leftover text in the buffer, save it too
</span><span>    </span><span style="color:#b48ead;">if </span><span>buf:
</span><span>        chunks.</span><span style="color:#bf616a;">append</span><span>(buf)
</span><span>
</span><span>    </span><span style="color:#b48ead;">return </span><span>chunks
</span><span>
</span></code></pre>
<hr />
<h2 id="diagram-how-it-fits-together">Diagram: How It Fits Together</h2>
<p>So far we have produced markdown of the complete housing act. We've then split it up into chunks. Next up we will need to ingest this data into our vector database. When our database is populated we can start work on retrieving data with our LLM. Here’s a simple flow of the system we’re building:</p>
<p><img src="/images/llm-mermaid.png" alt="img" /></p>
<hr />
<h2 id="to-be-continued-part-2">To be continued (Part 2)</h2>
<p>In <strong>Part 2</strong>, we’ll connect the dots:</p>
<ul>
<li>Retrieve relevant chunks with Chroma.</li>
<li>Feed them into Ollama.</li>
<li>Get answers with <strong>citations</strong> to the exact law paragraphs.</li>
</ul>
<p>This turns the raw Housing Act into a <strong>mini legal AI assistant</strong> you can run on your own machine.</p>
<blockquote>
<p><sup>1</sup> I hate the term <strong>“hallucinate”</strong> here. Models aren’t taking LSD and seeing pink elephants — they’re just <strong>wrong</strong>. Call it “fabricating”, “lying”, or “predicting nonsense if you squint too hard”. 🔗 <a href="https://blog.scottlogic.com/2024/09/10/llms-dont-hallucinate.html">Anything but hallucinating</a>.</p>
</blockquote>

</main>

    <div class="dark-mode-buttons">
        <button class="dark-mode-button" id="dark-mode-on"><img src="https://elvim.cc/dark_mode.svg" width="24" height="24" alt="Dark mode" aria-label="dark mode toggle" title="Dark mode"></button>
        <button class="dark-mode-button" id="dark-mode-off"><img src="https://elvim.cc/light_mode.svg" width="24" height="24" alt="Light mode" aria-label="light mode toggle" title="Light mode"></button>
    </div>
    <script>
        const cls = document.querySelector("html").classList;
        const sessionTheme = sessionStorage.getItem("theme");

        function setDark() {
            cls.add("dark-mode");
            cls.remove("light-mode");
            sessionStorage.setItem("theme", "dark");
        }
        function setLight() {
            cls.add("light-mode");
            cls.remove("dark-mode");
            sessionStorage.setItem("theme", "light");
        }

        if (sessionTheme === "dark") {
            setDark();
        } else if (sessionTheme === "light") {
            setLight();
        } else if (window.matchMedia("(prefers-color-scheme: dark)").matches) {
            setDark();
        }

        document.getElementById("dark-mode-on").addEventListener("click", function(e) {
            setDark();
        });
        document.getElementById("dark-mode-off").addEventListener("click", function(e) {
            setLight();
        });
    </script>
    <noscript>
        <style>
            .dark-mode-buttons {
                display: none;
            }
        </style>
    </noscript>
</body>
</html>
